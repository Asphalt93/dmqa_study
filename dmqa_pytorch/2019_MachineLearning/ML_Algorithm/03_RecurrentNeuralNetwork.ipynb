{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Series\n",
    "\n",
    "<br>\n",
    "<span style=\"color:gray\">\n",
    "\n",
    "1. Neural Net - part.1\n",
    "\n",
    "\n",
    "2. Convolution Neural Network\n",
    "\n",
    "\n",
    "3. Neural Net - part.2\n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "<b>\n",
    "\n",
    "4. Recursive Nerural Network\n",
    "\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Recurrent Neural Network ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 RNN이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation in Recurrent Neural Network Layer\n",
    "\n",
    "<img src=\"img/RNN_01.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters :  27\n",
      "txt_data_size :  27\n"
     ]
    }
   ],
   "source": [
    "txt_data = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "\n",
    "chars = list(set(txt_data))\n",
    "\n",
    "num_chars = len(chars) #the number of unique characters\n",
    "txt_data_size = len(txt_data)\n",
    "\n",
    "print(\"unique characters : \", num_chars)\n",
    "print(\"txt_data_size : \", txt_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'u': 0, 'a': 1, 'w': 2, 'j': 3, 'p': 4, 'f': 5, 'h': 6, 'z': 7, 'n': 8, 't': 9, ' ': 10, 'q': 11, 'g': 12, 'r': 13, 's': 14, 'e': 15, 'v': 16, 'd': 17, 'x': 18, 'c': 19, 'm': 20, 'l': 21, 'y': 22, 'k': 23, 'b': 24, 'o': 25, 'i': 26} \n",
      "\n",
      "{0: 'u', 1: 'a', 2: 'w', 3: 'j', 4: 'p', 5: 'f', 6: 'h', 7: 'z', 8: 'n', 9: 't', 10: ' ', 11: 'q', 12: 'g', 13: 'r', 14: 's', 15: 'e', 16: 'v', 17: 'd', 18: 'x', 19: 'c', 20: 'm', 21: 'l', 22: 'y', 23: 'k', 24: 'b', 25: 'o', 26: 'i'} \n",
      "\n",
      "[1, 24, 19, 17, 15, 5, 12, 6, 26, 3, 23, 21, 20, 8, 25, 4, 11, 13, 14, 9, 0, 16, 2, 18, 22, 7, 10] \n",
      "\n",
      "data length:  27\n"
     ]
    }
   ],
   "source": [
    "# one hot encode\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print(char_to_int,\"\\n\")\n",
    "print(int_to_char,\"\\n\")\n",
    "\n",
    "integer_encoded = [char_to_int[i] for i in txt_data]\n",
    "print(integer_encoded, \"\\n\")\n",
    "print(\"data length: \",len(integer_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 10000\n",
    "sequence_length = 10\n",
    "batch_size = round((txt_data_size / sequence_length) + 0.5)\n",
    "hidden_size = 100\n",
    "learning_rate = 1e-1\n",
    "\n",
    "W_xh = np.random.randn(hidden_size, num_chars)*0.01\n",
    "W_hh = np.random.randn(hidden_size, hidden_size)*0.01\n",
    "W_hy = np.random.randn(num_chars, hidden_size)*0.01\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
    "b_y = np.zeros((num_chars, 1)) # output bias\n",
    "\n",
    "h_prev = np.zeros((hidden_size, 1)) # h_(t-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(inputs, targets, h_prev):\n",
    "        \n",
    "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
    "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
    "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
    "    loss = 0 # loss initialization\n",
    "    \n",
    "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
    "        \n",
    "        xs[t] = np.zeros((num_chars,1)) \n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh( np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h ) # hidden state. \n",
    "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
    "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
    " \n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
    "        print(loss)\n",
    "    return loss, ps, hs, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/RNN_03.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(ps, inputs, hs, xs):\n",
    "\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
    "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
    "\n",
    "    # reversed\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
    "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy \n",
    "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(W_hh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dbh, dby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 69.105512\n",
      "iter 1000, loss: 0.002753\n",
      "iter 2000, loss: 0.001419\n",
      "iter 3000, loss: 0.000985\n",
      "iter 4000, loss: 0.000759\n",
      "iter 5000, loss: 0.000618\n",
      "iter 6000, loss: 0.000522\n",
      "iter 7000, loss: 0.000454\n",
      "iter 8000, loss: 0.000402\n",
      "iter 9000, loss: 0.000360\n"
     ]
    }
   ],
   "source": [
    "data_pointer = 0\n",
    "\n",
    "# memory variables for Adagrad\n",
    "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
    "\n",
    "\n",
    "for i in range(iteration):\n",
    "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    data_pointer = 0 # go from start of data\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        \n",
    "        inputs = [char_to_int[ch] for ch in txt_data[data_pointer:data_pointer+sequence_length]]\n",
    "        targets = [char_to_int[ch] for ch in txt_data[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
    "            \n",
    "        if (data_pointer+sequence_length+1 >= len(txt_data) and b == batch_size-1): # processing of the last part of the input data. \n",
    "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
    "\n",
    "\n",
    "        # forward\n",
    "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
    "\n",
    "        \n",
    "        # backward\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs) \n",
    "        \n",
    "        \n",
    "    # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam # elementwise\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
    "    \n",
    "        data_pointer += sequence_length # move data pointer\n",
    "        \n",
    "    if i % 1000 == 0:\n",
    "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_char, length):\n",
    "    x = np.zeros((num_chars, 1)) \n",
    "    x[char_to_int[test_char]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((hidden_size,1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
    "        y = np.dot(W_hy, h) + b_y\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) \n",
    "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
    "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
    "        x = np.zeros((num_chars, 1)) # init\n",
    "        x[ix] = 1 \n",
    "        ixes.append(ix) # list\n",
    "    txt = ''.join(int_to_char[i] for i in ixes)\n",
    "    print ('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " bcdefghijk \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "predict('a',10) # (char, len of output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " bdxqrs opq \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "predict('b',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "EPOCHS = 10\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.Field의 인자들\n",
    "\n",
    "sequential ::: 데이터셋이 순차적인 데이터셋인지 명시해준다. 이 때 LABEL 값은 단순히 클래스를 \n",
    "\n",
    "batch_first ::: 파리미터로 신경망에 입력되는 텐서의 첫 번째 차원값이 batch_size가 되도록 정해준다.\n",
    "\n",
    "lower ::: 텍스트 데이터 속 모든 영문 알파벳이 소문자가 되도록 처리해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에러 내용 :: RuntimeError: multi-target not supported :: model을 train할 때 발생\n",
    "\n",
    "원인 :: LABEL Field의 sequential 인자를 True로 지정함.\n",
    "sequential을 True로 지정하게 되면 Tokenization이 진행됨.\n",
    "> sequential – Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trainset, min_freq=5)\n",
    "LABEL.build_vocab(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_freq ::: 학습데이터에서 최소 5번 이상 등장한 단어만 사전에 추가한다. 그 외에는 unk라는 토큰으로 대체한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset = trainset.split(split_ratio=0.8)\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits((trainset, valset, testset),\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            shuffle=True, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.BucketIterator ::: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[학습셋]: %d [검증셋]: %d [테스트셋]: %d [단어수]: %d [클래스]: %d\" % (len(trainset), len(valset), len(testset), vocab_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRNN(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "        super(BasicRNN, self).__init__()\n",
    "        print(\"Building Basic RNN model\")\n",
    "        \n",
    "        self.n_layers = n_layers #아주 복잡한 모델이 아닌 이상 n_layers는 2이하로 설정한다.\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim) #\n",
    "        self.hidden_dim = hidden_dim #RNN을 통해 형성되는 은닉벡터의 차원값\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.rnn = nn.RNN(embed_dim, self.hidden_dim,\n",
    "                         num_layers=self.n_layers,\n",
    "                         batch_first=True)\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        h_0 = self._init_state(batch_size=x.size(0))\n",
    "        x, _ = self.rnn(x, h_0)\n",
    "        h_t = x[:,-1,:]\n",
    "        self.dropout(h_t)\n",
    "        logit = self.out(h_t)\n",
    "        return logit\n",
    "\n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_() #zero_()는 텐서 내 모든 값을 0으로 초기화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding<br>\n",
    "임베딩(embedding)은 자연어를 숫자의 나열인 벡터로 바꾼 결과 혹은 그 일련의 과정 전체를 가리키는 용어입니다. \n",
    "단어나 문장 각각을 벡터로 변환해 벡터 공간에 ‘끼워 넣는다(embed)’는 취지에서 임베딩이라는 이름이 붙었습니다. \n",
    "컴퓨터가 자연어를 처리할 수 있게 하려면 자연어를 계산 가능한 형식인 임베딩으로 바꿔줘야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Out<br>\n",
    "드롭아웃 기법은 뉴런의 연결을 임의로 삭제하는 것이다. 훈련할 때 임의의 뉴런을 골라 삭제하여 신호를 전달하지 않게 한다. 테스트할 때는 모든 뉴런을 사용한다. 신경망이 깊어짐에 따라 발생하는 over-fitting 문제를 해결하기 위함이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    \"\"\"evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    for batch in val_iter:\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1) # 레이블값을 0과 1로 변환\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y, reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicRNN(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = None\n",
    "for e in range(1, EPOCHS+1):\n",
    "    train(model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
    "    \n",
    "    print(\"[이폭: %d] 검증 오차:%5.2f | 검증 정확도:%5.2f\" % (e, val_loss, val_accuracy))\n",
    "    \n",
    "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "    \n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\")\n",
    "        torch.save(model.state_dict(),\n",
    "                  './snapshot/txtclassification.pt')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT (Back Propagation Through Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/RNN_02.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN의 $h$는 '상태(state)'를 기억해 시각이 1 스텝 (1단위;1t) 진행될 때마다 $h = \\text{tanh}(h_{t-1}W_h + x_tW_x + b)$의 형태로 갱신이 된다.\n",
    "\n",
    "보통 RNN의 출력 $h_t$를 은닉 상태(hidden state) 혹은 은닉 상태 벡터(hidden state vector)라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Vanishing & Gradient Exploding problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long-Short Term Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU (Gate Recurrent Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmqa_pytorch",
   "language": "python",
   "name": "dmqa_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
