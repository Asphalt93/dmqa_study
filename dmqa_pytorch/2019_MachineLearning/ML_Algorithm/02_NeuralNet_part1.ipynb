{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Series\n",
    "\n",
    "\n",
    "\n",
    "1. <b>Neural Net - part.1</b>\n",
    "\n",
    "<span style=\"color:gray\">\n",
    "\n",
    "2. Convolution Neural Network\n",
    "\n",
    "\n",
    "3. Neural Net - part.2\n",
    "\n",
    "\n",
    "4. Recursive Nerural Network\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퍼셉트론(Perceptron)\n",
    "퍼셉트론은 다수의 신호를 input(입력)으로 받아 하나의 output(출력)을 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 예시는 input(입력)이 2개인 퍼셉트론이다.\n",
    "\n",
    "<img src=\"img/NN_A_10.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아주 기본적인 퍼셉트론은 뉴런에서 보내온 신호의 총합이 정해진 한계(임계값; $\\theta$)를 넘어설 때만 1을 출력한다. (즉, 뉴런이 활성화되는 경우에만!)\n",
    "\n",
    "여기서 쓰인 개념은 다음과 같다.\n",
    "\n",
    "- $w_1$, $w_2$ ::: 가중치를 나타내는 매개변수로, 각 신호의 영향력을 제어한다.\n",
    "- $b$ ::: 편향을 나타내는 매개변수로, 뉴런이 얼마나 쉽게 활성화되는가를 제어한다.\n",
    "\n",
    "<u>다만 이번 예시에서 심플한 설명을 위해 Bias(편향)은 쓰지 않는다.</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 이 구조를 수식으로 표현하면 아래와 같다.\n",
    "\n",
    "$$y = \\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                   0 \\hspace{5mm} (w_1x_1 + w_2x_2 \\leq \\theta)\\\\\n",
    "                   1 \\hspace{5mm} (w_1x_1 + w_2x_2 > \\theta)\\\\\n",
    "                \\end{array}\n",
    "              \\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 논리회로 (AND, NAND, OR, XOR)\n",
    "\n",
    "입력(input)과 출력(output)의 대응 표를 진리표(truth table)라고 한다.\n",
    "\n",
    "각 게이트 별 진리표는 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AND 게이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_02.PNG\">\n",
    "\n",
    "AND 게이트는 두 입력이 모두 1인 경우에만 1을 출력하고 나머지는 0을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]] 에 대한 AND게이트의 결과는... \n",
      "\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def AND(elem_matrix):\n",
    "    w1 = np.ones((1,1)) * 0.5\n",
    "    w2 = np.ones((1,1)) * 0.5\n",
    "    w = np.concatenate((w1,w2))\n",
    "    \n",
    "    theta = np.ones((4,1)) * 0.7\n",
    "    \n",
    "    cal_result = (np.dot(elem_matrix.T, w) > theta).astype(int)\n",
    "    return cal_result\n",
    "\n",
    "elem_input = np.array([[0,1,0,1],\n",
    "                       [0,0,1,1]])\n",
    "\n",
    "print(elem_input.T,'에 대한 AND게이트의 결과는...','\\n')\n",
    "print(AND(elem_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAND 게이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_03.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]] 에 대한 NAND게이트의 결과는... \n",
      "\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def NAND(elem_matrix):\n",
    "    w1 = np.ones((1,1)) * -0.5 # AND와 부호를 반대로 하여 가중치를 부여한다.\n",
    "    w2 = np.ones((1,1)) * -0.5 # AND와 부호를 반대로 하여 가중치를 부여한다.\n",
    "    w = np.concatenate((w1,w2))\n",
    "    \n",
    "    b = np.ones((4,1)) * 0.7 # Bias; 편향\n",
    "    \n",
    "    theta = np.ones((4,1)) * 0\n",
    "    \n",
    "    cal_result = (np.dot(elem_matrix.T, w) + b > theta).astype(int)\n",
    "    return cal_result\n",
    "\n",
    "elem_input = np.array([[0,1,0,1],\n",
    "                       [0,0,1,1]])\n",
    "\n",
    "print(elem_input.T,'에 대한 NAND게이트의 결과는...','\\n')\n",
    "print(NAND(elem_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OR 게이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_04.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]] 에 대한 OR게이트의 결과는... \n",
      "\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def OR(elem_matrix):\n",
    "    w1 = np.ones((1,1)) * 0.2 # AND와 부호를 반대로 하여 가중치를 부여한다.\n",
    "    w2 = np.ones((1,1)) * 0.2 # AND와 부호를 반대로 하여 가중치를 부여한다.\n",
    "    w = np.concatenate((w1,w2))\n",
    "    \n",
    "    b = np.ones((4,1)) * -0.2 # Bias; 편향\n",
    "    \n",
    "    theta = np.ones((4,1)) * 0\n",
    "    \n",
    "    cal_result = (np.dot(elem_matrix.T, w) + b >= theta).astype(int)\n",
    "    return cal_result\n",
    "\n",
    "elem_input = np.array([[0,1,0,1],\n",
    "                       [0,0,1,1]])\n",
    "\n",
    "print(elem_input.T,'에 대한 OR게이트의 결과는...','\\n')\n",
    "print(OR(elem_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XOR 게이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_05.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR 게이트(배타적 논리합 논리회로)는 $x_1$과 $x_2$ 중 한쪽이 1일 때만 1을 출력한다.\n",
    "\n",
    "<b>따라서 <span style=\"color:red\">기본적인 퍼셉트론으로는 XOR 게이트를 구현하는 것이 불가능하다.</span></b>\n",
    "\n",
    "이유는 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_06.PNG\">\n",
    "\n",
    "퍼셉트론을 구현한 코드를 살펴보면 알겠지만 AND, NAND, OR 게이트는 가중치와 편향만 다르게 부여한 같은 구조의 일차방정식인 것을 알 수 있다.\n",
    "\n",
    "따라서 그래프 상에서도 볼 수 있듯이 AND, NAND, OR게이트의 경우 직선으로 세모와 원을 구분한다. 하지만 XOR 게이트는 직선으로 해결할 수 없다.\n",
    "\n",
    "하지만 구역을 \"직선\"으로 나눠야한다는 제약을 버리게 된다면 \"비선형\"적인 방법으로 해결할 수 있습니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다층 퍼셉트론(Multi-Layer Perceptron)\n",
    "다층 퍼셉트론은 말그대로 다수(multi)의 퍼셉트론으로 층(layer)을 쌓아서 만드는 퍼셉트론이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR 게이트의 문제는 AND, NAND, OR 게이트를 조합하여 해결할 수 있다.\n",
    "\n",
    "아래와 같이, $x_1$과 $x_2$는 NAND와 OR 게이트에 input을 하고, 이 두 게이트의 output을 AND 게이트의 input으로 다시 집어넣는다.\n",
    "\n",
    "따라서 다음과 같은 truth table(진리표)를 얻게 된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_07.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]] 에 대한 XOR게이트의 결과는... \n",
      "\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def XOR(elem_matrix):\n",
    "    \n",
    "    s1 = NAND(elem_matrix).T\n",
    "    s2 = OR(elem_matrix).T\n",
    "    \n",
    "    s = np.concatenate((s1,s2))\n",
    "    \n",
    "    cal_result = (AND(s) > theta).astype(int)\n",
    "    return cal_result\n",
    "\n",
    "elem_input = np.array([[0,1,0,1],\n",
    "                       [0,0,1,1]])\n",
    "\n",
    "print(elem_input.T,'에 대한 XOR게이트의 결과는...','\\n')\n",
    "print(XOR(elem_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 과정을 그래프로 표현하면 아래와 같이 나타낼 수 있다.\n",
    "\n",
    "<img src=\"img/NN_A_08.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AND, NAND, OR 게이트가 1층인 단층 퍼셉트론이며, XOR 게이트는 층이 여러개(2층)인 다층 퍼셉트론이다.\n",
    "\n",
    "단일 퍼셉트론을 조합함으로써 단층 퍼셉트론으로는 표현하지 못한 것(비선형 구조)을 구현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인공신경망(ANN; Artifical Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 그렸던 Multi Layer Perceptron(MLP; 다층 퍼셉트론)은 아래와 같이 각각 \n",
    "- Input Layer (입력층) \n",
    "- Hidden Layer (은닉층)\n",
    "- Output Layer (출력층)\n",
    "\n",
    "...로 나타낼 수 있다.\n",
    "<img src=\"img/NN_A_09.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function (활성화 함수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맨 처음 그렸던 단순 퍼셉트론 그래프를 조금 수정해서 그리면 다음과 같다.\n",
    "\n",
    "<img src=\"img/NN_A_11.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">!!! 재구성 필요 !!!</span>\n",
    "\n",
    "여기서 activation function _ h()를 통해 계산되는 과정은 다음과 같다.\n",
    "\n",
    "$$y = h(b + w_1x_1 + w_2x_2)$$\n",
    "\n",
    "$$h(x) = \\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                   0 (x\\leq0)\\\\\n",
    "                   1 (x > 0)\\\\\n",
    "                \\end{array}\n",
    "              \\right.$$\n",
    "              \n",
    "h()는 계단함수로 임계값을 경계로 출력이 바뀐다.\n",
    "\n",
    "activation function에는 비단 계단함수만 있는 것이 아니라 sigmoid function, hyperbolic tangent 등 다양하게 존재한다.\n",
    "\n",
    "책에는 이렇게 적혀있다.<br>\n",
    "\"실은 활성화 함수를 계단 함수에서 다른 함수로 변경하는 것이 신경망의 세계로 나아가는 열쇠입니다!\"<br>\n",
    "출처 ::: 밑바닥부터 시작하는 딥러닝, p.68\n",
    "\n",
    "\n",
    "<b><span style=\"color:red\">+++ simple perceptron, MLP, NN을 구분하는 activation function에 대한 내용을 자세히 적어두기</span></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"일반적으로 <b>단순 퍼셉트론</b>은 단층 네트워크에서 <b>계단 함수</b> (임계값을 경계로 출력이 바뀌는 함수)를 활성화 함수로 사용한 모델을 가리키고\n",
    "\n",
    "<b>다층 퍼셉트론</b>은 <b>신경망</b> (여러 층으로 구성되고 시그모이드 함수 등의 매끈한 활성화 함수를 사용하는 네트워크)을 가리킨다.\"\n",
    "\n",
    "출처 ::: 밑바닥부터 시작하는 딥러닝, p.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">!!! 재구성 필요 !!!</span>\n",
    "\n",
    "밑바닥부터 시작하는 딥러닝, p.75 참고할 것.\n",
    "\n",
    "\n",
    "\n",
    "종류는 다양하지만 이들은 모두 비선형(non-linear) 함수라는 공통점을 가지고 있다. 선형 함수를 쓰지 않는 이유는 층을 깊게 해도 별 의미가 없기 때문이다.\n",
    "\n",
    "만약에 activation function이 $h(x) = cx$인 3층 네트워크를 식으로 구현하면 다음과 같다.\n",
    "\n",
    "$$ y(x) = h(h(h(x))) $$\n",
    "\n",
    "하지만 이 계산은  $y(x) = c * c * c * x$이기 때문에 $a = c^3$인 $y(x) = ax$와 똑같은 식이 되어버린다.\n",
    "\n",
    "따라서 굳이 은닉층이 없어도 표현할 수 있는 네트워크인 셈이다.\n",
    "\n",
    "그렇기 때문에 층을 쌓는 혜택을 얻고 싶다면 activation function은 non-linear 해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation (순전파)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_13.PNG\">\n",
    "\n",
    "<b>Forward propagation</b>은 <br>Input된 값이 Hidden Layer를 거치면서 Weight 업데이트와 Activation Function을 통해서 Output Layer로 결과값을 출력하는 것이다.\n",
    "\n",
    "Pytorch로 구현한 코드는 아래와 같다. 데이터는 MNIST를 사용하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784       # The image size = 28 x 28 = 784\n",
    "hidden_size = 500      # The number of nodes at the hidden layer\n",
    "num_classes = 10       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 5         # The number of times entire dataset is trained\n",
    "batch_size = 100       # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./mdata',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./mdata',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) #Full Connection\n",
    "        self.relu = nn.ReLU() #Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) #Full Connection\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.fc1 $\\rightarrow$ <b>1st Full-Connected Layer:</b> 784 (input data) >>> 500 (hidden node)<br>\n",
    "\\::: 784개의 특성을 가진 데이터 x와 500개의 특성을 가진 결과 y를 가지는 Linear 모델을 생성한다.\n",
    "\n",
    "\n",
    "self.relu $\\rightarrow$ <b>Non-Linear Relu Layer:</b> max(0,x)<br>\n",
    "\\::: 비선형성을 부여하기 위한 활성화 함수 ReLU. 만일 활성화함수가 없다면 일반 선형회귀모델이 되어버린다.\n",
    "\n",
    "\n",
    "self.fc2 $\\rightarrow$ <b>2nd Full-Connected Layer:</b> 500 (hidden node) >>> 10 (output class) <br>\n",
    "\\::: 500개의 특성을 가진 데이터 x와 10개의 특성을 가진 결과 y를 가지는 Linear 모델을 생성한다.\n",
    "***\n",
    "** 추가1<br>\n",
    "<span style=\"color:red\">Classification에 필요한 <b>Softmax function Layer</b>는 \n",
    "<br>이후 Loss function으로 쓰는 <b>nn.CrossEntropyLoss()에 포함</b>이 되어있기 때문에 따로 레이어를 추가해주지 않아도 된다.</span>\n",
    "\n",
    "nn.CrossEntropyLoss()는 nn.LogSoftmax()와 nn.NLLLoss()를 묶은 criterion이다.\n",
    "\n",
    "참조사이트 : https://medium.com/@zhang_yang/understanding-cross-entropy-implementation-in-pytorch-softmax-log-softmax-nll-cross-entropy-416a2b200e34\n",
    "\n",
    "<br>\n",
    "\n",
    "** 추가2<br>\n",
    "nn.BCELoss $\\rightarrow$ lastest layer need a sigmoid function<br>\n",
    "nn.BCEwithLogitsLoss $\\rightarrow$ not need a sigmoid function in lastest layer\n",
    "\n",
    "<br>\n",
    "\n",
    "** 추가3 $\\hspace{1cm}$ (밑바닥부터 시작하는 딥러닝 P.176)<br>\n",
    "신경망은 추론할 때는 마지막 Affine 계층의 출력을 인식 결과로 이용한다.<br>\n",
    "또한, 신경망에서 정규화하지 않는 출력결과(Softmax 앞의 Affine 계층의 출력)를 Score라 한다.<br>\n",
    "즉, 신경망 추론에서 답을 하나만 내는 경우에는 가장 높은 점수만 알면 되니 Softmax 계층은 필요 없다는 것이다.<br>\n",
    "반면, 신경망을 학습할 때에는 Softmax 계층이 필요하다.\n",
    "\n",
    "?? 그래서 레이어에 softmax function을 넣지 않고 CrossEntropyLoss()에 Softmax function이 같이 있는건가???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnet = FNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10K test images: 13 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28)) # Affine 계층 >> size를 (28, 28)에서 (784,)로 바꿔준다.\n",
    "    outputs = fnet(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10K test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_20.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fc1 레이어는 <b>torch.nn.Linear</b> 모델을 사용한다.\n",
    "\n",
    "$$ y = xA^T + b $$\n",
    "\n",
    "여기서 A는 weight 벡터를 의미한다.\n",
    "\n",
    "따라서 <b>nn.Linear(784, 500)</b>은\n",
    "\n",
    "    1) input에 사용하는 하나의 벡터에 원소가 784개 있으며\n",
    "    2) 하나의 원소에 500개의 weight를 적용하여 500개의 output을 만들겠다.\n",
    "\n",
    "...는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>fc1 레이어의 출력값</b>을 받는 수식은 다음과 같이 정리할 수 있다.\n",
    "\n",
    "$x$ : input value from input layer <br>\n",
    "$y$ : output of fc1 layer<br>\n",
    "$w$ : weight<br>\n",
    "$n$ : sequence of output value \n",
    "\n",
    "$ y_n = x_1w_{1n} + x_2w_{2n} + x_3w_{3n} + x_4w_{4n} + \\dots + x_{784}w_{784\\hspace{1mm}n} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fc1 레이어의 출력값은 activation function(여기서는 ReLU)을 통해서 비선형성을 가지게 되고 이는 다시 fc2 레이어의 input으로 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기에 임의로 설정된 파라미터를 사용한 Neural Network는 당연히 분류 성능(13%)이 낮을 수밖에 없다.\n",
    "\n",
    "따라서 Output 값의 Error를 활용하여 parameter를 재조정하게 된다. \n",
    "\n",
    "이때 Training을 위해서 Error 값을 Hidden Layer와 Input Layer로 다시 보내게 되는데 이를 <b>Backward propagation</b>(Back propagation; 역전파)이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation (역전파)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_14.PNG\">\n",
    "\n",
    "<b>Backward propagation</b>은 <br>Output Layer에서 출력된 결과 값을 통해서 다시 Input Layer 방향으로 Error 값을 보내면서 Weight를 다시 조정하는 것이다.\n",
    "\n",
    "Pytorch로 구현한 코드는 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Progapation을 통해서 Training하는 순서\n",
    "\n",
    "optimizer = torch.optim.SGD(fnet.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad() # Intialize the hidden weight to all zeros\n",
    "        outputs = fnet(images)\n",
    "        loss = criterion(outputs, labels) # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 7, 8, 9, 7, 8, 6, 4, 1,\n",
       "        9, 3, 8, 4, 4, 7, 0, 1, 9, 2, 8, 7, 8, 2, 6, 0, 6, 5, 3, 3, 3, 9, 1, 4,\n",
       "        0, 6, 1, 0, 0, 6, 2, 1, 1, 7, 7, 8, 4, 6, 0, 7, 0, 3, 6, 8, 7, 1, 5, 2,\n",
       "        4, 9, 4, 3, 6, 4, 1, 7, 2, 6, 5, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2,\n",
       "        3, 4, 5, 6])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1094, -1.2555,  0.9170, -0.9767, -0.1621, -0.1809,  2.3456, -1.2385,\n",
       "        -0.3987, -0.5186], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3514, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10K test images: 77 %\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = fnet(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10K test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy가 77%로 이전에 나왔던 13%보다 매우매우 좋아졌다. ~당연한거지만~\n",
    "\n",
    "이제 코드를 보면서 역전파가 진행되는 과정과 원리를 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>criterion = nn.CrossEntropyLoss()</b><br>\n",
    "...는 Cross Entropy를 사용해서 비용(Loss)을 구하는 함수이다.\n",
    "\n",
    "다만, <span style=\"color:red\">PyTorch는 기존(Ref.1 참고)과는 다른 방법을 사용</span>한다.\n",
    "\n",
    "$$ \\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)\n",
    "                       = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right) $$  <i>출처: PyTorch Docs.</i>\n",
    "                       \n",
    "\n",
    "위에서 언급했듯이 이는 nn.LogSoftmax()와 nn.NLLLoss()를 하나로 묶은 criterion이다.\n",
    "\n",
    "아래는 수식에 대한 설명이다.\n",
    "                       \n",
    "                       \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이미지 하나에 대한 Loss 계산 과정\n",
    "<img src=\"img/NN_A_22.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이미지 데이터셋에 대한 Loss 계산 과정\n",
    "<img src=\"img/NN_A_23_re2.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>optimizer = torch.optim.SGD(fnet.parameters(), lr=learning_rate)</b><br>\n",
    "...는 Stochastic Gradient Descent를 사용해서 최적해를 구하는 함수이다.\n",
    "\n",
    "$$ w_\\text{new} \\leftarrow w_\\text{old} - \\eta {{\\partial L} \\over {\\partial w} } $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_16.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 마디로 전체 데이터가 아닌 일부 데이터를 사용해서 최적해를 찾는 것이다.\n",
    "\n",
    "데이터가 큰 경우에는 Gradient Descent를 통해서 최적해를 찾을 때 학습이 오래 걸리게 된다. 왜냐하면 전체 데이터를 사용하기 때문이다.\n",
    "\n",
    "하지만 Stochastic Gradient Descent는 일부 데이터(Mini Batch)를 사용하기 때문에 상대적으로 빠르게 학습할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 역전파 계산\n",
    "\n",
    "Optimizer로 Stochastic Gradient Descent을 쓰기 때문에 미분을 통해서 Weight에 대한 Loss function의 기울기를 구해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 출력층 $\\rightarrow$ 은닉층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_24_re.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그래프에서 목표는 ${\\partial L}\\over {\\partial  w}$를 계산하는 것이다.\n",
    "\n",
    "하지만 이를 풀어서 표현하면 ${{\\partial Bok-Jab}\\over{\\partial w}} \\hspace{3mm}$이기 때문에 한 번에 계산하기가 매우 복잡하고 어렵다.\n",
    "\n",
    "따라서 <span style=\"color:blue\"><b>Chain Rule</b></span>을 사용하여 연속적인 미분의 곱으로 수식을 풀어서 표현한다.\n",
    "\n",
    "이는 다음과 같이 표현할 수 있다.\n",
    "\n",
    "$$ {{\\partial L} \\over {\\partial p}} * {{\\partial p} \\over {\\partial f}} * {{\\partial f} \\over {\\partial w}} = {{\\partial L} \\over {\\partial w}} $$\n",
    "\n",
    "이 수식을 하나씩 계산하면 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation @ nn.NLLLoss()\n",
    "\n",
    "$$ {{\\partial L} \\over {\\partial p}} = {{\\partial (-\\ln(p_k))} \\over {\\partial (p_k)}} = -{1 \\over p_k}$$\n",
    "\n",
    "여기서 $-\\ln()$는 Negative Log-Likelihood Loss function이고, <br>\n",
    "$p_k$는 정답클래스k의 Softmax 값이다.\n",
    "\n",
    "<br>\n",
    "\n",
    "** 참고 **<br>\n",
    "$y = \\ln x \\rightarrow y'={1 \\over x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation @ nn.LogSoftmax()\n",
    "\n",
    "$$ {{\\partial p} \\over {\\partial f}} = {{\\partial} \\over {\\partial (f_k)}}\\left({{e_{f_k}}\\over{\\sum_{j}e^{f_j}}}\\right) \\\\ = {{(e^{f_k})'\\sum_{j}e^{f_j} - (\\sum_{j}e^{f_j})'e^{f_k}}\\over{(\\sum_{j}e^{f_j})^2}} \\\\ = {{ e^{f_k}\\sum_{j}e^{f_j} - e^{f_k}e^{f_k} }\\over{(\\sum_{j}e^{f_j})^2}} $$\n",
    "\n",
    "$({\\sum_{j}e^{f_j}})'$가 $e^{f_k}$가 되는 이유는 $e^{f_k}$를 제외한 $e^{f_j}$는 모두 상수처리가 되어 미분시에 0이 되기 때문이다.\n",
    "\n",
    "$$ ={ {e^{f_k}\\left( {\\sum_{j}e^{f_j}} - e^{f_k} \\right)} \\over {(\\sum_{j}e^{f_j}})^2} \\\\ = {e^{f_k} \\over {\\sum_{j}e^{f_j}}} \\times \\left({1 - {e^{f_k} \\over {\\sum_{j}e^{f_j}}}}\\right) \\\\ = p_k(1-p_k)$$\n",
    "\n",
    "${e^{f_k} \\over {\\sum_{j}e^{f_j}}}$는 정답클래스k의 Softmax 값이기 때문에 $p_k$로 표현할 수 있기 때문이다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "** 참고 **<br>\n",
    "$\\left(  f(x) \\over g(x)  \\right)'$ = ${{g(x)f'(x) - f(x)g'(x)} \\over g(x)^2}$\n",
    "\n",
    "$\\left( e^x \\right)'$ = $e^x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation @ nn.Linear(500, 10)\n",
    "\n",
    "$${ {\\partial f_k} \\over {\\partial w_{kj}} } = { {\\partial} \\over {\\partial w_{kj}} }\\left(  h_1w_{k1} + h_2w_{k2} + \\dots + h_jw_{kj} + \\dots + h_{500}w_{k\\hspace{1mm}500}  \\right) \\\\ = h_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** 출력층 $\\rightarrow$ 은닉층 간 역전파 계산 결과\n",
    "\n",
    "$${{\\partial L} \\over {\\partial w}} = {{\\partial L} \\over {\\partial p}} * {{\\partial p} \\over {\\partial f}} * {{\\partial f} \\over {\\partial w}} \\\\ = - {1 \\over p_k} * p_k(1-p_k) * h_j \\\\ = (p_k - 1)h_j$$\n",
    "\n",
    "<br>\n",
    "\n",
    "따라서 Chain Rule에 의한 계산 결과는 <span style=\"color:blue\"> $ (p_k - 1)h_j $</span>이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** 출력층 $\\rightarrow$ 은닉층 간 $w$ 업데이트\n",
    "\n",
    "$$ w_\\text{new} \\leftarrow w_\\text{old} - \\eta(p_k - 1)h_j $$\n",
    "\n",
    "여기서 $\\eta$는 Learning Rate를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 은닉층 $\\rightarrow$ 입력층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_25_re3.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation @ nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{ReLU}(x) = \\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                   0 \\hspace{3mm} (x\\leq0)\\\\\n",
    "                   x \\hspace{3mm} (x > 0)\\\\\n",
    "                \\end{array}\n",
    "              \\right. \\hspace{1cm} \\rightarrow \\hspace{1cm} {{\\partial \\hspace{1mm} \\text{ReLU}(x)} \\over {\\partial x}} = \\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                   0 \\hspace{3mm} (x\\leq0)\\\\\n",
    "                   1 \\hspace{3mm} (x > 0)\\\\\n",
    "                \\end{array}\n",
    "              \\right. $$\n",
    "<br>\n",
    "\n",
    "따라서 가중치 업데이트는 다음과 같이 이루어진다.\n",
    "<br>              \n",
    "           $${ w_{r_{new}} } = \\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                   {w_{r_{old}}} \\hspace{3mm} (x\\leq0)\\\\\n",
    "                   {w_{r_{old}}} - \\eta\\sum{{\\partial L} \\over {\\partial w}} \\hspace{3mm} (x > 0)\\\\\n",
    "                \\end{array}\n",
    "              \\right.$$\n",
    "              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation @ nn.Linear(784, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${{\\partial w_{r}} \\over {\\partial w_{i1}}} = { {\\partial} \\over {\\partial w_{i1}} }\\left( \\sum_{i = 1}^{784} I_i*w_{i1}  \\right)  = I_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** 은닉층 $\\rightarrow$ 입력층 간 역전파 계산 결과\n",
    "\n",
    "$$ {{\\partial L} \\over {\\partial w_{i1}}} = {{\\partial L} \\over {\\partial w}} * {{\\partial w} \\over {\\partial w_{r}}} * {{\\partial w_{r}} \\over {\\partial w_{i1}}} \\\\ = {{\\partial L} \\over {\\partial w_{r}}} * 1 * { {\\partial} \\over {\\partial w_{i1}} }\\left( \\sum_{i = 1}^{784} I_i*w_{i1}  \\right) \\\\ = (p_k - 1)h_j * I_1$$\n",
    "\n",
    "따라서 Chain Rule에 의한 계산 결과는 <span style=\"color:blue\"> $ (p_k - 1)h_j * I_1$</span>이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** 은닉층 $\\rightarrow$ 입력층 간 $w$ 업데이트\n",
    "\n",
    "$$ w_{i1\\hspace{1mm}\\text{new}} \\leftarrow w_{i1\\hspace{1mm}\\text{old}} - \\eta(p_k - 1)h_jI_1 $$\n",
    "\n",
    "여기서 $\\eta$는 Learning Rate를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ref.1\n",
    "\n",
    "많이 알려진 Cross Entropy의 식은 아래와 같다.\n",
    "\n",
    "\n",
    "$$ H(P,Q) = -\\sum_{x}P(x)\\log Q(x) $$\n",
    "\n",
    "\n",
    "현재 사용하고 있는 MNIST 데이터를 분류한다고 하였을 때 계산하는 과정은 다음과 같다.\n",
    "\n",
    "<img src=\"img/NN_A_15.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 PyTorch는 nn.CrossEntropyLoss()에서 왜 위의 수식을 쓰지 않고 nn.LogSoftmax()와 nn.NLLLoss()의 조합을 쓰는 것일까?<br>\n",
    "$\\rightarrow$ Ref.2 참고하세욧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ref.2\n",
    "\n",
    "PyTorch에서 Negative Log-Likelihood Loss와 Cross Entropy Loss 간의 차이점<br>\n",
    "(출처 : https://discuss.pytorch.org/t/difference-between-cross-entropy-loss-or-log-likelihood-loss/38816/2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmqa_pytorch",
   "language": "python",
   "name": "dmqa_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
