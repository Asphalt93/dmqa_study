{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Series\n",
    "\n",
    "\n",
    "\n",
    "1. <b>Neural Net</b>\n",
    "\n",
    "<span style=\"color:gray\">\n",
    "\n",
    "2. Convolution Neural Network\n",
    "\n",
    "\n",
    "3. Recursive Nerural Network\n",
    "\n",
    "\n",
    "4. Activation Function</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퍼셉트론(Perceptron)\n",
    "퍼셉트론은 다수의 신호를 input(입력)으로 받아 하나의 output(출력)을 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 예시는 input(입력)이 2개인 퍼셉트론이다.\n",
    "\n",
    "<img src=\"img/NN_A_10.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아주 기본적인 퍼셉트론은 뉴런에서 보내온 신호의 총합이 정해진 한계(임계값; $\\theta$)를 넘어설 때만 1을 출력한다. (즉, 뉴런이 활성화되는 경우에만!)\n",
    "\n",
    "여기서 쓰인 개념은 다음과 같다.\n",
    "\n",
    "- $w_1$, $w_2$ ::: 가중치를 나타내는 매개변수로, 각 신호의 영향력을 제어한다.\n",
    "- $b$ ::: 편향을 나타내는 매개변수로, 뉴런이 얼마나 쉽게 활성화되는가를 제어한다.\n",
    "\n",
    "<u>다만 이번 예시에서 심플한 설명을 위해 Bias(편향)은 쓰지 않는다.</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 이 구조를 수식으로 표현하면 아래와 같다.\n",
    "\n",
    "$$  y = 0 \\ \\ \\ \\ (w_1x_1 + w_2x_2 \\leq \\theta) $$\n",
    "$$  y = 1 \\ \\ \\ \\ (w_1x_1 + w_2x_2 > \\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 논리회로 (AND, NAND, OR, XOR)\n",
    "\n",
    "입력(input)과 출력(output)의 대응 표를 진리표(truth table)라고 한다.\n",
    "\n",
    "각 게이트 별 진리표는 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AND 게이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_02.PNG\">\n",
    "\n",
    "AND 게이트는 두 입력이 모두 1인 경우에만 1을 출력하고 나머지는 0을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]] 에 대한 AND게이트의 결과는... \n",
      "\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def AND(elem_matrix):\n",
    "    w1 = np.ones((1,1)) * 0.5\n",
    "    w2 = np.ones((1,1)) * 0.5\n",
    "    w = np.concatenate((w1,w2))\n",
    "    \n",
    "    theta = np.ones((4,1)) * 0.7\n",
    "    \n",
    "    cal_result = (np.dot(elem_matrix.T, w) > theta).astype(int)\n",
    "    return cal_result\n",
    "\n",
    "elem_input = np.array([[0,1,0,1],\n",
    "                       [0,0,1,1]])\n",
    "\n",
    "print(elem_input.T,'에 대한 AND게이트의 결과는...','\\n')\n",
    "print(AND(elem_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAND 게이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_03.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]] 에 대한 NAND게이트의 결과는... \n",
      "\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def NAND(elem_matrix):\n",
    "    w1 = np.ones((1,1)) * -0.5 # AND와 부호를 반대로 하여 가중치를 부여한다.\n",
    "    w2 = np.ones((1,1)) * -0.5 # AND와 부호를 반대로 하여 가중치를 부여한다.\n",
    "    w = np.concatenate((w1,w2))\n",
    "    \n",
    "    b = np.ones((4,1)) * 0.7 # Bias; 편향\n",
    "    \n",
    "    theta = np.ones((4,1)) * 0\n",
    "    \n",
    "    cal_result = (np.dot(elem_matrix.T, w) + b > theta).astype(int)\n",
    "    return cal_result\n",
    "\n",
    "elem_input = np.array([[0,1,0,1],\n",
    "                       [0,0,1,1]])\n",
    "\n",
    "print(elem_input.T,'에 대한 NAND게이트의 결과는...','\\n')\n",
    "print(NAND(elem_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OR 게이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_04.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]] 에 대한 OR게이트의 결과는... \n",
      "\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def OR(elem_matrix):\n",
    "    w1 = np.ones((1,1)) * 0.2 # AND와 부호를 반대로 하여 가중치를 부여한다.\n",
    "    w2 = np.ones((1,1)) * 0.2 # AND와 부호를 반대로 하여 가중치를 부여한다.\n",
    "    w = np.concatenate((w1,w2))\n",
    "    \n",
    "    b = np.ones((4,1)) * -0.2 # Bias; 편향\n",
    "    \n",
    "    theta = np.ones((4,1)) * 0\n",
    "    \n",
    "    cal_result = (np.dot(elem_matrix.T, w) + b >= theta).astype(int)\n",
    "    return cal_result\n",
    "\n",
    "elem_input = np.array([[0,1,0,1],\n",
    "                       [0,0,1,1]])\n",
    "\n",
    "print(elem_input.T,'에 대한 OR게이트의 결과는...','\\n')\n",
    "print(OR(elem_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XOR 게이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_05.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR 게이트(배타적 논리합 논리회로)는 $x_1$과 $x_2$ 중 한쪽이 1일 때만 1을 출력한다.\n",
    "\n",
    "<b>따라서 <span style=\"color:red\">기본적인 퍼셉트론으로는 XOR 게이트를 구현하는 것이 불가능하다.</span></b>\n",
    "\n",
    "이유는 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_06.PNG\">\n",
    "\n",
    "퍼셉트론을 구현한 코드를 살펴보면 알겠지만 AND, NAND, OR 게이트는 가중치와 편향만 다르게 부여한 같은 구조의 일차방정식인 것을 알 수 있다.\n",
    "\n",
    "따라서 그래프 상에서도 볼 수 있듯이 AND, NAND, OR게이트의 경우 직선으로 세모와 동그라미를 구분한다. 하지만 XOR 게이트는 직선으로 해결할 수 없다.\n",
    "\n",
    "하지만 구역을 \"직선\"으로 나눠야한다는 제약을 버리게 된다면 \"비선형\"적인 방법으로 해결할 수 있습니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다층 퍼셉트론(Multi-Layer Perceptron)\n",
    "다층 퍼셉트론은 말그대로 다수(multi)의 퍼셉트론으로 층(layer)을 쌓아서 만드는 퍼셉트론이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR 게이트의 문제는 AND, NAND, OR 게이트를 조합하여 해결할 수 있다.\n",
    "\n",
    "아래와 같이, $x_1$과 $x_2$는 NAND와 OR 게이트에 input을 하고, 이 두 게이트의 output을 AND 게이트의 input으로 다시 집어넣는다.\n",
    "\n",
    "따라서 다음과 같은 truth table(진리표)를 얻게 된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_07.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]] 에 대한 XOR게이트의 결과는... \n",
      "\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def XOR(elem_matrix):\n",
    "    \n",
    "    s1 = NAND(elem_matrix).T\n",
    "    s2 = OR(elem_matrix).T\n",
    "    \n",
    "    s = np.concatenate((s1,s2))\n",
    "    \n",
    "    cal_result = (AND(s) > theta).astype(int)\n",
    "    return cal_result\n",
    "\n",
    "elem_input = np.array([[0,1,0,1],\n",
    "                       [0,0,1,1]])\n",
    "\n",
    "print(elem_input.T,'에 대한 XOR게이트의 결과는...','\\n')\n",
    "print(XOR(elem_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 과정을 그래프로 표현하면 아래와 같이 나타낼 수 있다.\n",
    "\n",
    "<img src=\"img/NN_A_08.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AND, NAND, OR 게이트가 1층인 단층 퍼셉트론이며, XOR 게이트는 층이 여러개(2층)인 다층 퍼셉트론이다.\n",
    "\n",
    "단일 퍼셉트론을 조합함으로써 단층 퍼셉트론으로는 표현하지 못한 것(비선형 구조)을 구현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인공신경망(ANN; Artifical Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 그렸던 Multi Layer Perceptron(MLP; 다층 퍼셉트론)은 아래와 같이 각각 \n",
    "- Input Layer (입력층) \n",
    "- Hidden Layer (은닉층)\n",
    "- Output Layer (출력층)\n",
    "\n",
    "...로 나타낼 수 있다.\n",
    "<img src=\"img/NN_A_09.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function (활성화 함수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맨 처음 그렸던 단순 퍼셉트론 그래프를 조금 수정해서 그리면 다음과 같다.\n",
    "\n",
    "<img src=\"img/NN_A_11.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">!!! 재구성 필요 !!!</span>\n",
    "\n",
    "여기서 activation function _ h()를 통해 계산되는 과정은 다음과 같다.\n",
    "\n",
    "$$y = h(b + w_1x_1 + w_2x_2)$$\n",
    "\n",
    "$$h(x) = \\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                   0 (x\\leq0)\\\\\n",
    "                   1 (x > 0)\\\\\n",
    "                \\end{array}\n",
    "              \\right.$$\n",
    "              \n",
    "h()는 계단함수로 임계값을 경계로 출력이 바뀐다.\n",
    "\n",
    "activation function에는 비단 계단함수만 있는 것이 아니라 sigmoid function, hyperbolic tangent 등 다양하게 존재한다.\n",
    "\n",
    "책에는 이렇게 적혀있다.<br>\n",
    "\"실은 활성화 함수를 계단 함수에서 다른 함수로 변경하는 것이 신경망의 세계로 나아가는 열쇠입니다!\"<br>\n",
    "출처 ::: 밑바닥부터 시작하는 딥러닝, p.68\n",
    "\n",
    "\n",
    "<b><span style=\"color:red\">+++ simple perceptron, MLP, NN을 구분하는 activation function에 대한 내용을 자세히 적어두기</span></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"일반적으로 <b>단순 퍼셉트론</b>은 단층 네트워크에서 <b>계단 함수</b> (임계값을 경계로 출력이 바뀌는 함수)를 활성화 함수로 사용한 모델을 가리키고\n",
    "\n",
    "<b>다층 퍼셉트론</b>은 <b>신경망</b> (여러 층으로 구성되고 시그모이드 함수 등의 매끈한 활성화 함수를 사용하는 네트워크)을 가리킨다.\"\n",
    "\n",
    "출처 ::: 밑바닥부터 시작하는 딥러닝, p.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">!!! 재구성 필요 !!!</span>\n",
    "\n",
    "밑바닥부터 시작하는 딥러닝, p.75 참고할 것.\n",
    "\n",
    "\n",
    "\n",
    "종류는 다양하지만 이들은 모두 비선형(non-linear) 함수라는 공통점을 가지고 있다. 선형 함수를 쓰지 않는 이유는 층을 깊게 해도 별 의미가 없기 때문이다.\n",
    "\n",
    "만약에 activation function이 $h(x) = cx$인 3층 네트워크를 식으로 구현하면 다음과 같다.\n",
    "\n",
    "$$ y(x) = h(h(h(x))) $$\n",
    "\n",
    "하지만 이 계산은  $y(x) = c * c * c * x$이기 때문에 $a = c^3$인 $y(x) = ax$와 똑같은 식이 되어버린다.\n",
    "\n",
    "따라서 굳이 은닉층이 없어도 표현할 수 있는 네트워크인 셈이다.\n",
    "\n",
    "그렇기 때문에 층을 쌓는 혜택을 얻고 싶다면 activation function은 non-linear 해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation (순전파)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_13.PNG\">\n",
    "\n",
    "<b>Forward propagation</b>은 <br>Input된 값이 Hidden Layer를 거치면서 Weight 업데이트와 Activation Function을 통해서 Output Layer로 결과값을 출력하는 것이다.\n",
    "\n",
    "Pytorch로 구현한 코드는 아래와 같다. 데이터는 MNIST를 사용하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784       # The image size = 28 x 28 = 784\n",
    "hidden_size = 500      # The number of nodes at the hidden layer\n",
    "num_classes = 10       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 5         # The number of times entire dataset is trained\n",
    "batch_size = 100       # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./mdata',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./mdata',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnet = FNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10K test images: 8 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = fnet(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10K test images: {} %'.format(100 * correct/ total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기에 임의로 설정된 파라미터를 사용한 Neural Network는 당연히 분류 성능이 낮을 수밖에 없다.  ~8%면 줄세운거보다 못하잖아?~\n",
    "\n",
    "따라서 Output 값의 Error를 활용하여 parameter를 재조정하게 된다. \n",
    "\n",
    "이때 Training을 위해서 Error 값을 Hidden Layer와 Input Layer로 다시 보내게 되는데 이를 <b>Backward propagation</b>(Back propagation; 역전파)이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation (역전파)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_14.PNG\">\n",
    "\n",
    "<b>Backward propagation</b>은 <br>Output Layer에서 출력된 결과 값을 통해서 다시 Input Layer 방향으로 Error 값을 보내면서 Weight를 다시 조정하는 것이다.\n",
    "\n",
    "Pytorch로 구현한 코드는 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Progapation을 통해서 Training하는 순서\n",
    "\n",
    "optimizer = torch.optim.SGD(fnet.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = fnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10K test images: 77 %\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = fnet(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10K test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy가 77%로 이전에 나왔던 8%보다 매우매우 좋아졌다. ~당연한거지만~\n",
    "\n",
    "이제 코드를 보면서 역전파가 진행되는 과정과 원리를 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>optimizer = torch.optim.SGD(fnet.parameters(), lr=learning_rate)</b><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>criterion = nn.CrossEntropyLoss()</b><br>\n",
    "...는 Cross Entropy를 사용해서 비용(Loss)을 구하는 함수이다.\n",
    "\n",
    "Cross Entropy의 식은 아래와 같다.\n",
    "\n",
    "\n",
    "$$ H(P,Q) = -\\sum_{x}P(x)\\log Q(x) $$\n",
    "\n",
    "\n",
    "현재 사용하고 있는 MNIST 데이터를 분류한다고 하였을 때 계산하는 과정은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NN_A_15.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmqa_pytorch",
   "language": "python",
   "name": "dmqa_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
