{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Series\n",
    "\n",
    "\n",
    "<span style=\"color:gray\">\n",
    "<br>\n",
    "\n",
    "1. Neural Net - part.1\n",
    "\n",
    "</span>\n",
    "\n",
    "2. <b>Convolution Neural Network</b>\n",
    "\n",
    "\n",
    "<span style=\"color:gray\">\n",
    "\n",
    "3. Neural Net - part.2\n",
    "\n",
    "\n",
    "4. Recursive Nerural Network\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Convolutional Neural Network ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution은 '합성곱'으로 . 따라서 CNN은 합성곱을 하는 신경망이다.\n",
    "\n",
    "Convolution Layer의 특징은 데이터의 형상을 유지해준다는 점이다.\n",
    "\n",
    "이전에 사용한 Linear Layer(full connected layer)의 경우, 어떤 차원의 데이터든 1차원 데이터로 평탄화하기 때문에 데이터의 형상이 무시되었다. \n",
    "\n",
    "하지만 Convolution Layer의 경우 3차원으로 입력 받은 데이터를 3차원으로 출력할 수 있기 때문에 데이터 형상을 유지할 수 있다.\n",
    "\n",
    "따라서 CNN이 이미지 분석에 강한 이유도, 이미지 데이터에는 RGB 값이나 픽셀 거리에 따른 연관성 여부 등 다양한 정보가 고차원으로 담겨있는데 Linear Layer는 이를 1차원으로 압축해버리기 때문에 상대적으로 Convolution Layer에 비해 정보의 손실이 많이 발생하기 때문이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 256\n",
    "learning_rate = 0.0002\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dset.MNIST(\"./\", train=True, transform=transforms.ToTensor(),\n",
    "                        target_transform=None, download = True)\n",
    "mnist_test = dset.MNIST(\"./\", train=False, transform=transforms.ToTensor(),\n",
    "                        target_transform=None, download = True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of Convolution Neural Network\n",
    "\n",
    "<img src=\"img/CNN_05.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(64*3*3, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer(x)\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = self.fc_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer\n",
    "<img src=\"img/CNN_01.PNG\">\n",
    "\n",
    "다음의 이미지를 참고해서 설명을 진행하겠다.\n",
    "\n",
    "<img src=\"img/CNN_04.PNG\">\n",
    "\n",
    "고먐미의 저주에 걸린 고양이를 분류하는 모델을 만든다고 하자.\n",
    "\n",
    "고먐미는 고양이에 비해 직각으로 각진 부분이 많다. \n",
    "<br>따라서 다음 예시와 같이 생긴 필터는 고양이보다 고먐미 이미지 데이터에서 더 합성곱의 값을 출력할 것이다.\n",
    "\n",
    "이런 방식으로 feature map을 만들어서 분류 모델을 학습한다.\n",
    "\n",
    "인자에 대한 설명은 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* in_channels \n",
    "    - 현재 input한 데이터의 채널 수이다.\n",
    "\n",
    "\n",
    "* out_channels  \n",
    "    - 필터를 적용해서 만든 데이터의 개수 (여기선 필터 16개를 한 데이터에 적용해서 16개의 결과물을 만들어내었다.)\n",
    "    - <span style=\"color:red\"><b>??? 근데 channel을 줄일 떄는 어떻게하지???? 하나의 채널로 합친다음 다시 채널을 나누는건가</b>....밑바닥부터시작하는딥러닝 p.234 참고 </span>\n",
    "    \n",
    "\n",
    "* kernel_size\n",
    "    - 커널 사이즈는 필터의 크기로 만일 2로 설정된다면 2x2 사이즈의 필터가 생성된다.\n",
    "\n",
    "\n",
    "<img src=\"img/CNN_03.PNG\">\n",
    "\n",
    "* stride\n",
    "    - 출력크기를 조정할 목적으로 사용된다.\n",
    "    - 필터를 적용하는 위치의 간격으로, 스트라이드의 수만큼 필터가 이동하게 된다.\n",
    "\n",
    "\n",
    "* padding \n",
    "    - 출력 크기를 조정할 목적으로 사용된다.\n",
    "    - 입력 데이터 주변을 특정 값(예컨대 0)으로 채우는 방법이 있다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution leverages three important ideas that can help improve a machine learning system\n",
    "\n",
    "1. Sparse interactions\n",
    "\n",
    "2. Parameter sharing\n",
    "\n",
    "3. Equivariant representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sparse interactions\n",
    "\n",
    "<img src=\"img/CNN_06.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Parameter sharing\n",
    "\n",
    "<img src=\"img/CNN_07.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Equivariant representations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "<img src=\"img/CNN_02.PNG\">\n",
    "\n",
    "* Pooling Layer ?\n",
    "    - pooling은 데이터 크기를 조정할 목적으로 사용된다. (세로/가로 방향의 공간을 줄이는 연산)\n",
    "    - 종류에는 Max pooling, Average pooling 등이 있다.\n",
    "\n",
    "\n",
    "\n",
    "* Pooling Layer의 특징\n",
    "    - 채널 수에 영향을 주지 않는다.\n",
    "    - 입력의 변화에 영향을 적게 받는다. (Robust하다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092dc22046884374a10eec4f02ac8043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_arr = []\n",
    "for i in tqdm_notebook(range(num_epoch)):\n",
    "    for j,[image, label] in enumerate(train_loader):\n",
    "        x = image.to(device)\n",
    "        y_= label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)\n",
    "        loss = loss_func(output, y_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if j%1000 == 0:\n",
    "            print(loss)\n",
    "            loss_arr.append(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data: 98.53765869140625\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image,label in test_loader:\n",
    "        x = image.to(device)\n",
    "        y_= label.to(device)\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        _, output_index = torch.max(output, 1)\n",
    "        \n",
    "        total += label.size(0)\n",
    "        correct += (output_index == y_).sum().float()\n",
    "    \n",
    "    print(\"Accuracy of Test Data: {}\".format(100*correct/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmqa_pytorch",
   "language": "python",
   "name": "dmqa_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
