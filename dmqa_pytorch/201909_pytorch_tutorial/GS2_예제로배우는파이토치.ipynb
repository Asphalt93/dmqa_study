{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N은 배치 크기이며, D_in은 입력의 차원입니다.\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위로 가중치를 초기화합니다.\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100) (100, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    w1.shape,\n",
    "    w2.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0146681946648067e-08\n",
      "100 3.453226572882835e-11\n",
      "200 1.3123300121477219e-13\n",
      "300 5.600151799602955e-16\n",
      "400 2.653116518733606e-18\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # 손실(loss)을 계산하고 출력합니다.\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t%100 == 0:\n",
    "        print(t, loss)\n",
    "    \n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred) #벡터미분\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T) #벡터미분\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h) #벡터미분\n",
    "    \n",
    "    # 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N은 배치 크기이며, D_in은 입력의 차원입니다.\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#무작위로 가중치를 초기화합니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[선형대수 연산]\n",
    "1. dot : 벡터의 내적\n",
    "2. mv : 행렬과 벡터의 곱\n",
    "3. mm : 행력과 행렬의 곱\n",
    "4. matmul : 인수의 종류에 따라서 자동으로 dot, mv, mm을 선택\n",
    "\n",
    "[기타 연산]\n",
    "1. clamp(min, max) : input 되는 값의 상한/하한을 설정하면 그 기준으로 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 692.4354248046875\n",
      "199 3.5412392616271973\n",
      "299 0.02804386243224144\n",
      "399 0.0004838769091293216\n",
      "499 6.411041249521077e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.mm(w1) #행렬과 행렬의 곱\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # 손실(loss)을 계산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "    \n",
    "    # 손실에 따른 w1, w2의 변화도를 계싼하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "# requires_grad = False로 설정하여 역전파 중에 이 Tensor들에 대한 변화도를 계산할\n",
    "# 필요가 없음을 나타냅니다.\n",
    "# requires_grad의 기본값은False이다..\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype, requires_grad=False)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "# requires_grad=True로 설정하여 역전파 중에 이 Tensor들에 대한\n",
    "# 변화도를 계산할 필요가 있음을 나타냅니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1229.3853759765625\n",
      "199 11.426041603088379\n",
      "299 0.14126911759376526\n",
      "399 0.002246161224320531\n",
      "499 0.00016095700266305357\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: Tensor 연산을 사용하여 예상되는 y값을 계산합니다.\n",
    "    # 이는 Tensor를 사용한 순전파 단계와 완전히 동일하지만, 역전파 단계를 별도로 구현하지 않아도 되므로\n",
    "    # 중간값들에 대한 참조를 갖고 있을 필요가 없습니다.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Tensor 연산을 사용하여 손실을 계산하고 출력합니다.\n",
    "    # loss는 (1, ) 형태의 Tensor이며, loss.item()은 loss의 스칼라 값입니다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # autograd를 사용하여 역전파 단계를 계산합니다.\n",
    "    # 이는 requires_grad=True를 갖는 모든 Tensor에 대해 손실의 변화도를 계산합니다.\n",
    "    # 이후 w1.grad와 w2.grad는 w1과 w2 각각에 대한 소실의 변화도를 갖는 Tensor가 됩니다.\n",
    "    loss.backward()\n",
    "    \n",
    "    # 경사하강법을 사용하여 가중치를 수동으로 갱신합니다.\n",
    "    # torch.no_grad()로 감싸는 이유는 가중치들이 requires_grad=True이지만\n",
    "    # autograd에서는 이를 추적할 필요가 없기 때문입니다.\n",
    "    # tensor.data가 tensor의 저장공간을 공유하기는 하지만, 이력을 추적하진 않습니다.\n",
    "    # 또한, 이를 위해 torch.optim.SGD를 사용할 수도 있습니다.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmqa_pytorch",
   "language": "python",
   "name": "dmqa_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
