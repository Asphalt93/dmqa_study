{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8., 12.]) None None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(data=[2.0, 3.0], requires_grad = True)\n",
    "y = x**2\n",
    "z = 2*y + 3\n",
    "\n",
    "target = torch.tensor([3.0, 4.0])\n",
    "loss = torch.sum(torch.abs(z - target))\n",
    "loss.backward()\n",
    "\n",
    "print(x.grad, y.grad, z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss.backward()에서 연산 그래프를 쭉 따라가면서 잎 노드 x에 대한 기울기를 계산한다. <br>\n",
    "여기서 말하는 잎 노드는 다른 별수를 통해 계산되는 y나 z가 아니라 그 자체가 값인 x 같은 노드를 의미한다.<br>\n",
    "결과적으로 마지막 줄을 실행하면 x.grad는 기울기가 계산되지만 y.grad, z.grad는 잎 노드가 아니기 때문에 결과값이 None이 리턴된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.7983)\n",
      "tensor(3.0781)\n",
      "tensor(2.4785)\n",
      "tensor(2.0770)\n",
      "tensor(1.8238)\n",
      "2.017725944519043 2.400055170059204\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "num_data = 1000\n",
    "num_epoch = 500\n",
    "\n",
    "x = init.uniform_(torch.Tensor(num_data, 1), -10, 10)\n",
    "noise = init.normal_(torch.FloatTensor(num_data, 1), std=1)\n",
    "y = 2*x+3\n",
    "y_noise = 2*(x+noise)+3\n",
    "\n",
    "model = nn.Linear(1,1) #선형회귀모델 호출\n",
    "                       #우리가 만든 데이터x는 1개의 특성을 가진 데이터 1000개이고\n",
    "                       #결과y도 1개의 특성을 가진 데이터 1000개이기 때문에 인수로 모델 Linear(1,1)로 생성하였다.\n",
    "        \n",
    "loss_func = nn.L1Loss() #L1손실 (차이의 절댓값의 평균)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr =0.01)\n",
    "\n",
    "label = y_noise\n",
    "for i in range(num_epoch):\n",
    "    optimizer.zero_grad() #반복할 때마다 지난 반복에서 계산된 기울기를 0으로 초기화한다.\n",
    "                          #기울기를 초기화해야 새로운 가중치와 편차에 대해서 새로운 기울기를 구할 수 있기 때문이다.\n",
    "    output = model(x)\n",
    "    \n",
    "    loss = loss_func(output, label) #nn.L1Loss(output, label)\n",
    "    loss.backward() #각 변수에 대한 기울기가 계산된다.\n",
    "    optimizer.step() #model.parameters()에서 return되는 변수들의 기울기에 학습률을 곱하여 빼줌으로서 업데이트를 한다.\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(loss.data)\n",
    "param_list = list(model.parameters())\n",
    "print(param_list[0].item(), param_list[1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init.uniform_(텐서,-10,10)) ::: 텐서의 값을 -10부터 10까지 무작위로 설정한다. <br>\n",
    "init.normal_(텐서, std=1) ::: 표준편차 1을 가지는 정규분포로 초기화한다.<br>\n",
    "\n",
    "L1손실 ::: 차이의 절댓값의 평균 $$loss(x,y)={1 \\over n}\\Sigma|x_i-y_i|$$<br>\n",
    "데이터 전체를 학습에 한 번 사용하는 주기를 <b>에폭(epoch)</b>라고 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmqa_pytorch",
   "language": "python",
   "name": "dmqa_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
